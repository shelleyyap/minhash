{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removes everything except letters, apostrophes, whitespaces\n",
    "def clean_articles(text):\n",
    "    return ' '.join(re.sub('[^a-zA-Z\\' ]', ' ', text).split()).lower()\n",
    "\n",
    "# Tokenizes the text into trigrams\n",
    "def tokenize(text):\n",
    "    tokens = text.split()\n",
    "    num_words = len(tokens)\n",
    "    tokens_set = []\n",
    "    for num in range(0, num_words - 2):\n",
    "        word = tokens[num] + \" \" + tokens[num + 1] + \" \" + tokens[num + 2]\n",
    "        tokens_set.append(word)\n",
    "    return tokens_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of articles in training set: 100\n"
     ]
    }
   ],
   "source": [
    "# Read the articles, cleans and tokenize the text\n",
    "train_set = 'data/articles_100.train'\n",
    "with open(train_set, 'r') as f:\n",
    "    data = f.read().split('\\n')\n",
    "\n",
    "data_articles = {}\n",
    "for line in data:\n",
    "    if line:\n",
    "        newline = line.split(' ', 1)\n",
    "        data_articles[newline[0]] = tokenize(clean_articles(newline[1]))\n",
    "print 'Number of articles in training set:', len(data_articles)\n",
    "del line\n",
    "del newline\n",
    "del data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Indexes all the trigrams so that each trigram is represented by a unique integer.\n",
    "#word_set is {word : index}\n",
    "word_set = {}\n",
    "count = 1\n",
    "for key, value in data_articles.iteritems():\n",
    "    for word in value:\n",
    "        if word not in word_set:\n",
    "            word_set[word] = count\n",
    "            count = count + 1\n",
    "del count\n",
    "del word\n",
    "del key\n",
    "del value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns a list of size num_features, where the elements are random integers between 0 and max_shingle_size\n",
    "def pick_rand_coeff(max_shingle_size, num_features):\n",
    "    randlist = []\n",
    "    assert num_features < max_shingle_size, 'Feature size must be less than max shingle size'\n",
    "    for i in range(0, num_features):\n",
    "        rand_gen_int = random.randint(0, max_shingle_size)\n",
    "        while rand_gen_int in randlist:\n",
    "            rand_gen_int = random.randint(0, max_shingle_size)\n",
    "        randlist.append(rand_gen_int)\n",
    "    return randlist\n",
    "\n",
    "# Computes the hash function using the formula (ax + b) % c, where a and b are randomly generated integers and c is a \n",
    "# prime number slightly larger than max_shingle_size\n",
    "def hash_function(x, coef_a, coef_b, prime):\n",
    "    return (x * coef_a + coef_b) % prime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates the arrays for a and b\n",
    "arr_coeff_a = pick_rand_coeff(len(word_set), 10)\n",
    "arr_coeff_b = pick_rand_coeff(len(word_set), 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculates the minhashes for each document\n",
    "prime = 22003\n",
    "data_hashes = []\n",
    "for key, value in data_articles.iteritems():\n",
    "    signature = [key]\n",
    "    for hash_index in range(0, 10):\n",
    "        minimum = prime\n",
    "        for word in value:\n",
    "            hash_value = hash_function(word_set[word], arr_coeff_a[hash_index], arr_coeff_b[hash_index], prime)\n",
    "            if hash_value < minimum:\n",
    "                minimum = hash_value\n",
    "        signature.append(minimum)\n",
    "    data_hashes.append(signature)\n",
    "del key\n",
    "del value\n",
    "del signature\n",
    "del hash_index\n",
    "del minimum\n",
    "del word\n",
    "del hash_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 1: t1952 \n",
      "Document 2: t3495 \n",
      "Count of similar items: 10 \n",
      "\n",
      "Document 1: t1768 \n",
      "Document 2: t5248 \n",
      "Count of similar items: 9 \n",
      "\n",
      "Document 1: t1088 \n",
      "Document 2: t5015 \n",
      "Count of similar items: 10 \n",
      "\n",
      "Document 1: t980 \n",
      "Document 2: t2023 \n",
      "Count of similar items: 10 \n",
      "\n",
      "Document 1: t1297 \n",
      "Document 2: t4638 \n",
      "Count of similar items: 10 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Compares the minhashes between the documents and flags out similar documents based on the minhash\n",
    "threshold = 5\n",
    "results = []\n",
    "for i in range(0, len(data_hashes)):\n",
    "    key1 = data_hashes[i][0]\n",
    "    signature1 = data_hashes[i][1:]\n",
    "    \n",
    "    for j in range(i + 1, len(data_hashes)):\n",
    "        key2 = data_hashes[j][0]\n",
    "        signature2 = data_hashes[j][1:]\n",
    "    \n",
    "        count = 0\n",
    "        for signature in signature1:\n",
    "            if signature in signature2:\n",
    "                count = count + 1\n",
    "        if count > threshold:\n",
    "            if int(key1[1:]) > int(key2[1:]):\n",
    "                results.append(key2 + ' ' + key1)\n",
    "                print 'Document 1:', key2, '\\nDocument 2:', key1, '\\nCount of similar items:', count, '\\n'\n",
    "            else:\n",
    "                results.append(key1 + ' ' + key2)\n",
    "                print 'Document 1:', key1, '\\nDocument 2:', key2, '\\nCount of similar items:', count, '\\n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 1: t1088 \n",
      "Document 2: t5015 \n",
      "\n",
      "Document 1: t1297 \n",
      "Document 2: t4638 \n",
      "\n",
      "Document 1: t1768 \n",
      "Document 2: t5248 \n",
      "\n",
      "Document 1: t1952 \n",
      "Document 2: t3495 \n",
      "\n",
      "Document 1: t980 \n",
      "Document 2: t2023 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_set = 'data/articles_100.truth'\n",
    "with open(train_set, 'r') as f:\n",
    "    data = f.read().split('\\n')\n",
    "\n",
    "data_truth = []\n",
    "for pair in data:\n",
    "    if pair:\n",
    "        if int(pair.split()[0][1:]) > int(pair.split()[1][1:]):\n",
    "            data_truth.append(pair.split()[1] + ' ' + pair.split()[0])\n",
    "            print 'Document 1:', pair.split()[1], '\\nDocument 2:', pair.split()[0], '\\n'\n",
    "        else:\n",
    "            data_truth.append(pair.split()[0] + ' ' + pair.split()[1])\n",
    "            print 'Document 1:', pair.split()[0], '\\nDocument 2:', pair.split()[1], '\\n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 100.0 %\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for result in results:\n",
    "    if result in data_truth:\n",
    "        count = count + 1\n",
    "print 'Accuracy:', count / float(len(data_truth)) * 100, '%'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Permutation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_article = pd.DataFrame([])\n",
    "for key, article in data_articles.iteritems():\n",
    "    row_article = pd.DataFrame(np.zeros((1, len(matrix_article.columns))), columns = matrix_article.columns)\n",
    "    for word in article:\n",
    "        if word not in matrix_article.columns:\n",
    "            matrix_article[word] = 0\n",
    "        row_article[word] = 1\n",
    "    matrix_article = pd.concat([matrix_article, row_article])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
